{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m2DfpugeNWC0"
   },
   "source": [
    "# Face Recognition Project\n",
    "\n",
    "Akhil Jariwala, Isaac Hales, Laurel Hales <br>\n",
    "CS230 - Deep Learning <br>\n",
    "v0.2 - \n",
    "2020/05/23"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z-gvPLAt-3_5"
   },
   "source": [
    "## Notebook Overview\n",
    "\n",
    "This project takes photos, attempts to detect children (ages < 15) in the images, and then swaps each child face with a donor face of the same general age and gender.\n",
    "\n",
    "This process can be broken down into three main sections:\n",
    "\n",
    "* Face Detection\n",
    "* Age and Gender Detection\n",
    "* Face Swapping\n",
    "\n",
    "Each section is completed using a network specifically built for the given task.\n",
    "\n",
    "* Face Detection uses faced (https://github.com/iitzco/faced)\n",
    "* Gender Detection is currently done using the process described here: https://data-flair.training/blogs/python-project-gender-age-detection/.\n",
    "* We implemented age detection\n",
    "* Face Swapping is achieved using fsgan (https://github.com/YuvalNirkin/fsgan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e_NaPoeq_i4z"
   },
   "source": [
    "### Import Base Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "upZqUnJz_iJz"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import sys\n",
    "import random\n",
    "import shutil\n",
    "import math\n",
    "import argparse\n",
    "import pickle\n",
    "import base64\n",
    "from base64 import b64encode\n",
    "import tensorflow as tf\n",
    "# from google.colab.patches import cv2_imshow\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JqReWmOl2jCv"
   },
   "outputs": [],
   "source": [
    "\n",
    "base_dir = \"/home/ubuntu/cs230/\"\n",
    "\n",
    "# Face Swapping requires a temporary directory\n",
    "temp_dir = base_dir + \"temp/\"\n",
    "# !mkdir \"/home/ubuntu/cs230/temp/\"\n",
    "\n",
    "# Directory for gender and age detection files\n",
    "gad_directory = base_dir + \"gad/\"\n",
    "\n",
    "# Directory for test photos\n",
    "test_photo_dir = base_dir + \"familyPhotosCleanedBatch4/\"\n",
    "\n",
    "# Directory for fsgan\n",
    "fsgan_dir = base_dir + 'fsgan/projects/'\n",
    "\n",
    "# Donor Faces (from UTKFace dataset)\n",
    "donor_dir = base_dir + \"UTKFaceChildren/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4EITxGE5-q-U"
   },
   "source": [
    "## Setup Code for Each Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AKfhpPMB--3i"
   },
   "source": [
    "### Faced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 938
    },
    "colab_type": "code",
    "id": "OQlav2nxvc-9",
    "outputId": "f0aa6904-d23c-4a76-b5cc-2e3c2dab80ba"
   },
   "outputs": [],
   "source": [
    "# Setup steps - just run once\n",
    "#!git clone https://github.com/iitzco/faced.git\n",
    "\n",
    "# Updated faced to use TensorFlow 2\n",
    "# !git clone https://github.com/ihales/faced\n",
    "# !pip install /home/ubuntu/cs230/faced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g27Y6A9-Ah5F"
   },
   "source": [
    "### fsgan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mWzp_k4dAhpz"
   },
   "source": [
    "Acknowledgements - from the notebook in the fsgan repo: We thank Dr. Eyal Gruss, [wangchao0899](https://github.com/wangchao0899), [jjandnn](https://github.com/jjandnn), and [zhuhaozh](https://github.com/zhuhaozh) as well as the original author of the fsgan repository for helping with this demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup steps - just run once\n",
    "\n",
    "# !sudo apt-get install ffmpeg\n",
    "# !mkdir /home/ubuntu/cs230/fsgan/projects\n",
    "# !mkdir /home/ubuntu/cs230/fsgan/data\n",
    "# !pip install opencv-python ffmpeg-python yacs\n",
    "# !cd /home/ubuntu/cs230/fsgan/projects\n",
    "# !git clone https://github.com/YuvalNirkin/face_detection_dsfd\n",
    "# !git clone https://github.com/YuvalNirkin/fsgan.git\n",
    "# !conda install pytorch\n",
    "# !pip install torchvision\n",
    "\n",
    "sys.path += ['/usr/local/lib/python3.7/site-packages', base_dir + 'fsgan/projects']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "boXujDyIDjFK"
   },
   "source": [
    "#### Configure fsgan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QF_EjOlfBu04"
   },
   "outputs": [],
   "source": [
    "from fsgan.inference.swap import FaceSwapping\n",
    "from fsgan.criterions.vgg_loss import VGGLoss\n",
    "\n",
    "weights_dir = fsgan_dir + 'weights'\n",
    "finetune_iterations = 800\n",
    "seg_remove_mouth = True\n",
    "\n",
    "\n",
    "detection_model = os.path.join(weights_dir, 'WIDERFace_DSFD_RES152.pth')\n",
    "pose_model = os.path.join(weights_dir, 'hopenet_robust_alpha1.pth')\n",
    "lms_model = os.path.join(weights_dir, 'hr18_wflw_landmarks.pth')\n",
    "seg_model = os.path.join(weights_dir, 'celeba_unet_256_1_2_segmentation_v2.pth')\n",
    "reenactment_model = os.path.join(weights_dir, 'nfv_msrunet_256_1_2_reenactment_v2.1.pth')\n",
    "completion_model = os.path.join(weights_dir, 'ijbc_msrunet_256_1_2_inpainting_v2.pth')\n",
    "blending_model = os.path.join(weights_dir, 'ijbc_msrunet_256_1_2_blending_v2.pth')\n",
    "criterion_id_path = os.path.join(weights_dir, 'vggface2_vgg19_256_1_2_id.pth')\n",
    "criterion_id_path = os.path.join(weights_dir, 'vggface2_vgg19_256_1_2_id.pth')\n",
    "criterion_id = VGGLoss(criterion_id_path)\n",
    "\n",
    "\n",
    "face_swapping = FaceSwapping(\n",
    "    detection_model=detection_model, pose_model=pose_model, lms_model=lms_model,\n",
    "    seg_model=seg_model, reenactment_model=reenactment_model,\n",
    "    completion_model=completion_model, blending_model=blending_model,\n",
    "    criterion_id=criterion_id,\n",
    "    finetune=True, finetune_save=True, finetune_iterations=finetune_iterations,\n",
    "    seg_remove_mouth=finetune_iterations, batch_size=16, seg_batch_size=48,\n",
    "    encoder_codec='mp4v')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "01YbA_6qD6k0"
   },
   "source": [
    "#### Function to call fsgan and return a OpenCV image with swaped face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QU_fcS2bBuwS"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def setup_donor_face(age, gender):\n",
    "    \"\"\" Given and age classification and gender, return a donor face\"\"\"\n",
    "  \n",
    "    # convert gender encodings\n",
    "    gender = 1 if gender == \"f\" else 0\n",
    "\n",
    "    # filter through possible faces\n",
    "    filenames = os.listdir(donor_dir)\n",
    "\n",
    "    donor_file = random.choice([x for x in filenames if int(x.split(\"_\")[1]) == gender])\n",
    "    shutil.copyfile(donor_dir + donor_file, temp_dir + \"source_image.jpg\")\n",
    "\n",
    "def swap_face(original_face, age_and_gender):\n",
    "    \"\"\" Given an image, age classification, and gender, replaces the face with a donor face.\n",
    "\n",
    "    Arguments:\n",
    "    original_face -- an OpenCV image to use for swapping\n",
    "    age_and_gender -- a tuple with an integer age classification and a gender (\"m\" or \"f\")\n",
    "\n",
    "    Returns:\n",
    "    replacement_image -- an OpenCV image to use as a replacement in the original picture\n",
    "    \"\"\"\n",
    "    age = age_and_gender[0]\n",
    "    gender = age_and_gender[1]\n",
    "\n",
    "    # Quit early if not a minor\n",
    "    if age > 15:\n",
    "        return original_face\n",
    "\n",
    "\n",
    "    # Start by saving the image to disk\n",
    "    target_image_path = temp_dir + \"target_image.jpg\"\n",
    "    cv2.imwrite(target_image_path, original_face)\n",
    "    # Select a source image at random from directory of face donors\n",
    "    setup_donor_face(age, gender)\n",
    "    source_image_path = temp_dir + \"source_image.jpg\"\n",
    "\n",
    "    # Swap the faces\n",
    "    finetune = True\n",
    "    output_tmp_path = temp_dir + 'output_tmp.mp4'\n",
    "    face_swapping(source_image_path, target_image_path, output_tmp_path,\n",
    "                'longest', 'longest', finetune)\n",
    "\n",
    "    vidcap = cv2.VideoCapture(output_tmp_path)\n",
    "    success, image = vidcap.read()\n",
    "\n",
    "    # Remove Temp Files\n",
    "    os.remove(output_tmp_path)\n",
    "    os.remove(target_image_path)\n",
    "    os.remove(source_image_path)\n",
    "    shutil.rmtree(temp_dir + \"target_image/\",)\n",
    "    shutil.rmtree(temp_dir + \"source_image/\",)\n",
    "\n",
    "\n",
    "    return image\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i-a_gSc3yc-X"
   },
   "source": [
    "### Age and Gender Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jsgdRWnJzLqh"
   },
   "outputs": [],
   "source": [
    "# # Filenames and data for all the existing models and weights\n",
    "\n",
    "age_model = \"/home/ubuntu/cs230/saved_models/final_model\"\n",
    "\n",
    "genderProto=\"gender_deploy.prototxt\"\n",
    "genderModel=\"gender_net.caffemodel\"\n",
    "\n",
    "MODEL_MEAN_VALUES=(78.4263377603, 87.7689143744, 114.895847746)\n",
    "\n",
    "# List of options for output ages and genders\n",
    "genderList=['m','f']\n",
    "\n",
    "# Model definitions\n",
    "genderNet=cv2.dnn.readNet(gad_directory + genderModel, gad_directory + genderProto)\n",
    "ageNet = tf.keras.models.load_model(age_model, compile = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zZmnoWWozLk-"
   },
   "outputs": [],
   "source": [
    "def get_age_and_gender(face, padding=20):\n",
    "    IMG_SIZE=224\n",
    "    \n",
    "    blob=cv2.dnn.blobFromImage(face, 1.0, (227,227), MODEL_MEAN_VALUES, swapRB=False)\n",
    "    genderNet.setInput(blob)\n",
    "    genderPreds=genderNet.forward()\n",
    "    gender=genderList[genderPreds[0].argmax()]\n",
    "\n",
    "    ## This is where I will include our age detector\n",
    "\n",
    "    newImage = face.copy()\n",
    "    newImageRecolored = cv2.cvtColor(newImage, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Use `convert_image_dtype` to convert to floats in the [0,1] range.\n",
    "    img = tf.image.convert_image_dtype(newImageRecolored, tf.float32)\n",
    "\n",
    "    # resize the image to the desired size.\n",
    "    formatted_image = tf.image.resize(img, [IMG_SIZE, IMG_SIZE])\n",
    "    formatted_image = tf.reshape(formatted_image, [1, IMG_SIZE, IMG_SIZE, 3])\n",
    "    \n",
    "    age = ageNet.predict(formatted_image, steps=1)[0][0]\n",
    "\n",
    "    return (age, gender)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ytMlPT_TNPsL"
   },
   "source": [
    "### Initialize FaceDetector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "spzDIhRr55oL"
   },
   "outputs": [],
   "source": [
    "from faced import FaceDetector\n",
    "face_detector = FaceDetector()\n",
    "thresh = 0.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4W9O45I0ww9z"
   },
   "source": [
    "### Test Sample Classifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thanks to https://stackoverflow.com/a/54037086 for helping out with this one\n",
    "\n",
    "def byte_me(img):\n",
    "    _, buffer_img = cv2.imencode('.jpg', img)\n",
    "    data = base64.b64encode(buffer_img).decode(\"ascii\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zBMASR2Yw_Gu",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "photo_count = 0\n",
    "photo_facts_list = []\n",
    "\n",
    "photo_list = os.listdir(test_photo_dir)\n",
    "\n",
    "for photoName in photo_list:\n",
    "    photo_count += 1\n",
    "    print(\"Photo: \", photo_count)\n",
    "    ###\n",
    "    this_photo={\"id\": photo_count}\n",
    "\n",
    "    testImagePath = test_photo_dir + photoName\n",
    "    testImage = cv2.imread(testImagePath)\n",
    "\n",
    "    newImage = testImage.copy()\n",
    "    ###\n",
    "    this_photo[\"before\"] = byte_me(newImage)\n",
    "    this_photo[\"faces\"] = []\n",
    "\n",
    "\n",
    "    newImageRecolored = cv2.cvtColor(newImage, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Receives RGB numpy image (HxWxC) and\n",
    "    # returns (x_center, y_center, width, height, prob) tuples. \n",
    "    bboxes = face_detector.predict(newImageRecolored, thresh = thresh)\n",
    "\n",
    "    croppedFaces = []\n",
    "\n",
    "    #Set padding pixels for each photo to improve age detection algorithm\n",
    "    padding = 50\n",
    "\n",
    "    #Collect image width and height as maximums for bounding box x and y-coordinates\n",
    "    imageWidth = int(newImage.shape[1])\n",
    "    imageHeight = int(newImage.shape[0])\n",
    "\n",
    "    #Create a list for the adjusted face bounding boxes\n",
    "    faceBoundingBoxes = []\n",
    "\n",
    "    for box in bboxes:\n",
    "        # Extract x and y coordinates\n",
    "        xc, yc, wi, he, p = box\n",
    "        x1 = max(int(xc - wi//2 - padding),0)\n",
    "        x2 = min(int(xc + wi//2 + padding), imageWidth - 1)\n",
    "        x2 -= (x2-x1)%2\n",
    "        y1 = max(int(yc - he//2 - padding), 0)\n",
    "        y2 = min(int(yc + he//2 + padding), imageHeight - 1)\n",
    "        y2 -= (y2-y1)%2\n",
    "\n",
    "        paddedBox = [x1, x2, y1, y2]\n",
    "\n",
    "        faceBoundingBoxes.append(paddedBox)\n",
    "\n",
    "        #Add cropped face to list\n",
    "        croppedFace = newImage[y1:y2, x1:x2]\n",
    "        croppedFaces.append(croppedFace)\n",
    "\n",
    "    #Create a list for the swapped faces\n",
    "    swappedFaces = []\n",
    "\n",
    "    for face in croppedFaces:\n",
    "        this_face={\"before\": byte_me(face)}\n",
    "\n",
    "        # collect age and gender of face\n",
    "        age, gender = get_age_and_gender(face)\n",
    "\n",
    "        ###\n",
    "        this_face['age'] = age\n",
    "        this_face['gender'] = gender\n",
    "\n",
    "        # Generate swapped face\n",
    "        swapped_face = swap_face(face, (age,gender))\n",
    "\n",
    "\n",
    "        ###\n",
    "        this_face['after'] = byte_me(swapped_face)\n",
    "        this_photo['faces'].append(this_face)\n",
    "\n",
    "\n",
    "        # Add swapped face to new list of faces\n",
    "        swappedFaces.append(swapped_face)\n",
    "\n",
    "  \n",
    "    #Combine the faceBoundingBoxes and swappedFaces list to identify swappings to perform\n",
    "    swappingsToDo = list(zip(faceBoundingBoxes,swappedFaces))\n",
    "\n",
    "    for swapItem in swappingsToDo:\n",
    "        faceBoundingBox, replacementFace = swapItem\n",
    "        x1, x2, y1, y2 = faceBoundingBox\n",
    "\n",
    "        newImage[y1:y2, x1:x2] = replacementFace[0:y2-y1, 0:x2-x1]\n",
    "\n",
    "      \n",
    "    this_photo['after'] = byte_me(newImage)\n",
    "    photo_facts_list.append(this_photo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jcvAAEdh8DRR"
   },
   "outputs": [],
   "source": [
    "# Save the output to analyze later\n",
    "pickle.dump(photo_facts_list, open(\"photo_facts.p\", \"wb\"))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "DL Project - Face Recognition",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow2_p36)",
   "language": "python",
   "name": "conda_tensorflow2_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
